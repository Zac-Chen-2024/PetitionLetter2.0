# System Renovation Manual v2

## å…¨å±€ç›®æ ‡

å°†ç°æœ‰ç³»ç»Ÿä» L-1 paragraph-level pipeline å‡çº§ä¸º EB-1A sentence-level provenance ç³»ç»Ÿï¼ŒåŒæ—¶åŠ å…¥ snippet å…³è”ä¿¡å·å±‚ï¼Œæ”¯æ’‘è®ºæ–‡ Section 4.4 çš„æ‰€æœ‰æŠ€æœ¯å£°æ˜ã€‚

**æ”¹é€ èŒƒå›´ï¼š** åç«¯ pipeline + provenance engine + snippet linker | å‰ç«¯ WritingCanvas + DocumentViewer

**é¢„ä¼°æ€»å·¥æ—¶ï¼š** 5-7 å¤©

---

## ä¸€ã€ç°çŠ¶ vs ç›®æ ‡

### ç°æœ‰é“¾è·¯

```
OCR (deepseek_ocr)
  â†’ text_blocks [id, text, bbox_x1/y1/x2/y2, page]
    â†’ L1 analyzer â†’ quotes [quote_text, standard_key, page, exhibit_id]
      â†’ bbox_matcher â†’ quote â†” text_block åŒ¹é… [block_id, bbox, match_score]
        â†’ quote_index_map {idx: {exhibit_id, material_id, page, quote, bbox}}
          â†’ Writing (å•æ­¥) â†’ paragraph_text + citations_used [{exhibit_id, exhibit_title}]
```

### ç›®æ ‡é“¾è·¯

```
OCR (deepseek_ocr)
  â†’ text_blocks [id, text, bbox, page]                                    â† ä¸å˜ (Step 1)
    â†’ Snippet Extraction â†’ snippets [snippet_id, text, bbox, ...]         â† åŠ  ID (Step 2)
      â†’ Relationship Analyzer â†’ å®ä½“å›¾                                    â† å·²æœ‰ï¼Œæ‰©å±•å®ä½“ç±»å‹
        â†’ Snippet Linker â†’ snippet å…³è”ä¿¡å· [co-reference, ...]           â† æ–°å¢
          â†’ å¾‹å¸ˆæ‹–æ‹½æ˜ å°„ + å…³è”ä¿¡å·è¾…åŠ©                                     â† å‰ç«¯å·²æœ‰
            â†’ Writing (ä¸¤æ­¥æ‹†åˆ†)
              â†’ 3a è‡ªç”±å†™ä½œ (Claude Sonnet)                               â† æ–°å¢
              â†’ 3b å¥å­çº§æ ‡æ³¨ (GPT-4o-mini strict schema)                 â† æ–°å¢
                â†’ Hybrid Retrieval â†’ è¡¥å…¨/ä¿®æ­£ snippet_ids                â† æ–°å¢ (Step 4)
                  â†’ BBox Highlight â†’ ç‚¹å‡»å¥å­ â†’ é«˜äº® bbox                 â† å‰ç«¯æ–°å¢ (Step 5)
```

---

## äºŒã€API æ¶æ„ â€” å¤šæ¨¡å‹åˆ†å·¥

| ä»»åŠ¡ | æ¨¡å‹ | åŸå›  | è¾“å…¥è§„æ¨¡ |
|------|------|------|---------|
| Snippet Extraction | GPT-4o-mini | å®šå¼ã€strict schemaã€128K context | å•ä¸ª material 5-30K tokens |
| Relationship Analysis | GPT-4o-mini | å®ä½“æŠ½å–ã€å®šå¼ | åˆ†æ‰¹ï¼Œæ¯æ‰¹ ~5K tokens |
| Writing (3a) | Claude Sonnet | æ³•å¾‹å†™ä½œè´¨é‡æœ€å¥½ | å·²æ˜ å°„ snippets ~3-6K tokens |
| Annotation (3b) | GPT-4o-mini | strict JSON schema 100% åˆè§„ | æ®µè½ + snippets ~4-8K tokens |
| Snippet Linker | æ—  LLMï¼ˆå›¾ç®—æ³•ï¼‰ | ä»å®ä½“å›¾æ¨å¯¼ï¼Œé›¶æˆæœ¬ | å†…å­˜è®¡ç®— |

### Context é•¿åº¦åˆ†æ

**ç“¶é¢ˆåªåœ¨ Snippet Extractionã€‚** åç»­æ‰€æœ‰æ­¥éª¤çš„è¾“å…¥éƒ½æ˜¯å·²æå–çš„ snippetsï¼ˆå‡ åæ¡ã€æ¯æ¡ç™¾ä½™å­—ï¼‰ï¼Œä¸å†éœ€è¦å®Œæ•´ OCR æ–‡æœ¬ã€‚

```
é˜¶æ®µ              è¾“å…¥                         å¤§å°          æ˜¯å¦ç“¶é¢ˆ
OCR              PDF å›¾ç‰‡                      ä¸èµ° LLM      â€”
Snippet Extract  å•ä¸ª material çš„ OCR æ–‡æœ¬      5-30K tokens  â˜… å”¯ä¸€ç“¶é¢ˆ
Relationship     snippets åˆ†æ‰¹                  ~5K/æ‰¹        âœ“ å·²è§£å†³ï¼ˆç°æœ‰åˆ†æ‰¹é€»è¾‘ï¼‰
Writing 3a       å·²æ˜ å°„ snippets                ~3-6K         å®Œå…¨å¤Ÿ
Annotation 3b    æ®µè½ + snippets                ~4-8K         å®Œå…¨å¤Ÿ
Provenance       å¥å­ + snippets                ~2K           å®Œå…¨å¤Ÿ
```

**ç°æœ‰ material_splitter å·²ç»è§£å†³äº†å¤§ exhibit é—®é¢˜ï¼š** 200 é¡µ exhibit â†’ æ‹†æˆ 10-20 ä¸ª materials â†’ æ¯ä¸ª 5-20 é¡µ â†’ æ¯ä¸ª 5K-30K tokens â†’ GPT-4o-mini 128K context å®Œå…¨å¤Ÿã€‚

### Context ä¸å¤±çœŸç­–ç•¥

Snippet Extraction é˜¶æ®µï¼Œå®Œæ•´ OCR æ–‡æœ¬å¯èƒ½å«å¤§é‡æ ¼å¼å™ªå£°ã€‚ç°æœ‰ `clean_ocr_for_llm()` å·²åœ¨åšæ¸…æ´—ï¼Œä½†å¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–ï¼š

```python
def compress_ocr_for_extraction(ocr_text: str, max_tokens: int = 60000) -> str:
    """
    å‹ç¼© OCR æ–‡æœ¬ç”¨äº snippet extractionï¼Œä¿ç•™ä¿¡æ¯å¯†åº¦
    
    ç­–ç•¥ï¼š
    1. å»é™¤è¿ç»­ç©ºç™½è¡Œï¼ˆ>2 â†’ 1ï¼‰
    2. å»é™¤é¡µçœ‰é¡µè„šé‡å¤æ–‡æœ¬
    3. åˆå¹¶è·¨é¡µæ–­è¡Œ
    4. å¦‚æœä»è¶…é•¿ï¼ŒæŒ‰ text_block é‡è¦æ€§æ’åºæˆªæ–­
       - è¡¨æ ¼ã€æ•°å­—ã€äººåã€æ—¥æœŸ â†’ é«˜ä¼˜å…ˆ
       - ç©ºç™½ã€é¡µç ã€æ°´å° â†’ ä½ä¼˜å…ˆ
    """
    import re
    
    # 1. å‹ç¼©ç©ºç™½
    text = re.sub(r'\n{3,}', '\n\n', ocr_text)
    text = re.sub(r'[ \t]{3,}', ' ', text)
    
    # 2. å»é™¤é‡å¤çš„é¡µçœ‰é¡µè„š
    lines = text.split('\n')
    if len(lines) > 50:
        # ç»Ÿè®¡æ¯è¡Œå‡ºç°æ¬¡æ•°ï¼Œé«˜é¢‘è¡Œå¯èƒ½æ˜¯é¡µçœ‰é¡µè„š
        from collections import Counter
        line_counts = Counter(line.strip() for line in lines if line.strip())
        threshold = max(3, len(lines) // 20)  # å‡ºç°è¶…è¿‡ 5% çš„è¡Œ
        header_footer = {l for l, c in line_counts.items() if c >= threshold and len(l) < 100}
        lines = [l for l in lines if l.strip() not in header_footer]
        text = '\n'.join(lines)
    
    # 3. ç²—ç•¥ token ä¼°ç®—ï¼ˆ1 token â‰ˆ 4 chars è‹±æ–‡, â‰ˆ 1.5 chars ä¸­æ–‡ï¼‰
    estimated_tokens = len(text) // 3  # ä¿å®ˆä¼°è®¡
    if estimated_tokens <= max_tokens:
        return text
    
    # 4. è¶…é•¿æ—¶æˆªæ–­ï¼Œä¿ç•™å¤´å°¾
    keep_ratio = max_tokens / estimated_tokens
    char_limit = int(len(text) * keep_ratio)
    head = text[:char_limit * 2 // 3]
    tail = text[-(char_limit // 3):]
    return head + "\n\n[... middle section omitted for length ...]\n\n" + tail
```

---

## ä¸‰ã€åç«¯æ”¹é€ 

### Step 0ï¼šSnippet æ•°æ®æ¨¡å‹

ä¸ºæ¯ä¸ª quote ç”Ÿæˆç¨³å®š IDï¼Œå»ºç«‹ snippet æ³¨å†Œè¡¨ã€‚

**æ–°å»ºï¼š** `backend/app/services/snippet_registry.py`

```python
"""Snippet Registry â€” ä» L1/EB1A åˆ†æç»“æœæ„å»ºå¸¦ ID çš„ snippet æ³¨å†Œè¡¨"""

import hashlib
import json
from typing import List, Dict
from pathlib import Path

DATA_DIR = Path(__file__).parent.parent.parent / "data"
PROJECTS_DIR = DATA_DIR / "projects"


def generate_snippet_id(exhibit_id: str, page: int, quote_text: str) -> str:
    """åŸºäºå†…å®¹ç”Ÿæˆç¡®å®šæ€§ snippet_id"""
    content = f"{exhibit_id}:{page}:{quote_text[:100]}"
    hash_str = hashlib.md5(content.encode()).hexdigest()[:8]
    return f"snip_{hash_str}"


def build_registry(project_id: str, analyses: List[Dict]) -> List[Dict]:
    """
    ä»åˆ†æç»“æœæ„å»º snippet æ³¨å†Œè¡¨
    
    Args:
        analyses: L1/EB1A analyzer çš„è¾“å‡ºåˆ—è¡¨
    Returns:
        snippets: [{snippet_id, document_id, exhibit_id, material_id, 
                     text, page, bbox, standard_key, source_block_ids}]
    """
    snippets = []
    seen_ids = set()
    
    for doc_analysis in analyses:
        exhibit_id = doc_analysis.get("exhibit_id", "")
        document_id = doc_analysis.get("document_id", "")
        
        for q in doc_analysis.get("quotes", []):
            snippet_id = generate_snippet_id(
                exhibit_id, q.get("page", 0), q.get("quote", "")
            )
            if snippet_id in seen_ids:
                continue
            seen_ids.add(snippet_id)
            
            snippets.append({
                "snippet_id": snippet_id,
                "document_id": document_id,
                "exhibit_id": exhibit_id,
                "material_id": q.get("source", {}).get("material_id", ""),
                "text": q.get("quote", ""),
                "page": q.get("page"),
                "bbox": q.get("bbox"),
                "standard_key": q.get("standard_key", ""),
                "source_block_ids": q.get("matched_block_ids", [])
            })
    
    save_registry(project_id, snippets)
    return snippets


def save_registry(project_id: str, snippets: List[Dict]):
    path = PROJECTS_DIR / project_id / "snippets" / "registry.json"
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(snippets, f, ensure_ascii=False, indent=2)


def load_registry(project_id: str) -> List[Dict]:
    path = PROJECTS_DIR / project_id / "snippets" / "registry.json"
    if not path.exists():
        return []
    with open(path, 'r', encoding='utf-8') as f:
        return json.load(f)
```

**è§¦å‘æ—¶æœºï¼š** åœ¨ L1/EB1A åˆ†æ + `enrich_quotes_with_bbox()` å®Œæˆåè°ƒç”¨ `build_registry()`ã€‚

---

### Step 1ï¼šDual Indexing â€” ä¸å˜

`deepseek_ocr.py` â†’ TextBlock è¡¨ï¼ˆtext + bboxï¼‰ã€‚ç°çŠ¶å³ç›®æ ‡ã€‚

---

### Step 2ï¼šSnippet Extraction â€” å°æ”¹

ç°æœ‰ `l1_analyzer.py` + `quote_consolidator.py` å·²åœ¨åšã€‚æ”¹åŠ¨ä»…ä¸€å¤„ï¼šåˆ†æå®Œæˆåè°ƒç”¨ `snippet_registry.build_registry()`ã€‚

**EB-1A é€‚é…ï¼š** å¤åˆ¶ `l1_analyzer.py` â†’ `eb1a_analyzer.py`ï¼Œæ›¿æ¢ `L1_STANDARDS` ä¸º EB-1A 10 ä¸ªæ ‡å‡†ã€‚æˆ–è€…å°† standards é…ç½®åŒ–ï¼Œæ”¾åœ¨ project çº§åˆ«ã€‚

---

### Step 2.5ï¼ˆæ–°å¢ï¼‰ï¼šSnippet Linker â€” ä»å®ä½“å›¾æ¨å¯¼å…³è”ä¿¡å·

**æ–°å»ºï¼š** `backend/app/services/snippet_linker.py`

#### åŸç†

ç°æœ‰ `relationship_analyzer.py` äº§å‡ºå®ä½“å›¾ï¼šEntity(name, type) + Relation(from, to, type)ã€‚æ¯ä¸ªå®ä½“å’Œå…³ç³»éƒ½å¸¦æœ‰ `quote_refs`ï¼ˆsnippet ç´¢å¼•ï¼‰ã€‚

å¦‚æœä¸¤ä¸ª snippets æåˆ°äº†åŒä¸€ä¸ªå®ä½“ï¼Œå®ƒä»¬ä¹‹é—´å°±æœ‰ co-reference å…³è”ï¼š

```
snippet_003 â”€â”€æåˆ°â”€â”€â†’ Entity("Nature è®ºæ–‡") â†â”€â”€æåˆ°â”€â”€ snippet_007
  â†’ æ¨å¯¼ï¼šsnippet_003 â†” snippet_007 å…³è”ï¼ŒåŸå›  = å…±äº«å®ä½“ "Nature è®ºæ–‡"
```

**é›¶é¢å¤– LLM è°ƒç”¨ã€‚** çº¯å†…å­˜å›¾è®¡ç®—ã€‚

#### å®ç°

```python
"""
Snippet Linker â€” ä»å®ä½“å›¾æ¨å¯¼ snippet é—´å…³è”ä¿¡å·

è¾“å…¥ï¼šrelationship_analyzer äº§å‡ºçš„å®ä½“å›¾ + snippet_registry
è¾“å‡ºï¼šsnippet pairs + å…³è”ç±»å‹ + å…±äº«å®ä½“

ä¸è°ƒç”¨ LLMï¼Œçº¯å›¾ç®—æ³•ã€‚
"""

from typing import List, Dict, Tuple, Set
from collections import defaultdict
from pathlib import Path
import json

DATA_DIR = Path(__file__).parent.parent.parent / "data"
PROJECTS_DIR = DATA_DIR / "projects"


def build_snippet_links(
    graph_data: Dict,
    snippet_registry: List[Dict],
    min_shared_entities: int = 1
) -> List[Dict]:
    """
    ä»å®ä½“å›¾æ¨å¯¼ snippet å…³è”
    
    Args:
        graph_data: relationship_analyzer è¾“å‡º {entities, relations}
        snippet_registry: [{snippet_id, ...}]
        min_shared_entities: è‡³å°‘å…±äº«å‡ ä¸ªå®ä½“æ‰ç®—å…³è”
    
    Returns:
        links: [
            {
                "snippet_a": "snip_xxx",
                "snippet_b": "snip_yyy",
                "link_type": "co-reference",
                "shared_entities": ["Nature è®ºæ–‡", "Dr. Chen"],
                "strength": 0.8  # å…±äº«å®ä½“æ•° / ä¸¤ä¸ª snippet çš„å¹³å‡å®ä½“æ•°
            }
        ]
    """
    # å»ºç«‹ quote_ref â†’ snippet_id æ˜ å°„
    # quote_ref æ˜¯ relationship_analyzer é‡Œçš„ quote ç´¢å¼•
    # snippet_registry çš„é¡ºåºå’Œ quote ç´¢å¼•å¯¹é½
    idx_to_snippet = {}
    for i, s in enumerate(snippet_registry):
        idx_to_snippet[i] = s["snippet_id"]
    
    # å»ºç«‹ entity â†’ snippet_ids å€’æ’ç´¢å¼•
    entity_to_snippets: Dict[str, Set[str]] = defaultdict(set)
    snippet_entity_count: Dict[str, int] = defaultdict(int)
    
    entities = graph_data.get("entities", [])
    for entity in entities:
        entity_name = entity.get("name", "")
        entity_id = entity.get("id", "")
        quote_refs = entity.get("quote_refs", [])
        
        for ref in quote_refs:
            ref_int = int(ref)
            if ref_int in idx_to_snippet:
                sid = idx_to_snippet[ref_int]
                entity_to_snippets[entity_name].add(sid)
                snippet_entity_count[sid] += 1
    
    # éå†æ‰€æœ‰å®ä½“ï¼Œæ‰¾åˆ°å…±äº«åŒä¸€å®ä½“çš„ snippet pairs
    pair_shared: Dict[Tuple[str, str], List[str]] = defaultdict(list)
    
    for entity_name, snippet_ids in entity_to_snippets.items():
        snippet_list = sorted(snippet_ids)
        for i in range(len(snippet_list)):
            for j in range(i + 1, len(snippet_list)):
                pair_key = (snippet_list[i], snippet_list[j])
                pair_shared[pair_key].append(entity_name)
    
    # è¿‡æ»¤å¹¶ç”Ÿæˆ links
    links = []
    for (sa, sb), shared in pair_shared.items():
        if len(shared) < min_shared_entities:
            continue
        
        # è®¡ç®—å…³è”å¼ºåº¦ï¼šå…±äº«å®ä½“æ•° / ä¸¤ä¸ª snippet å¹³å‡å®ä½“æ•°
        avg_entities = (snippet_entity_count.get(sa, 1) + snippet_entity_count.get(sb, 1)) / 2
        strength = min(1.0, len(shared) / max(avg_entities, 1))
        
        links.append({
            "snippet_a": sa,
            "snippet_b": sb,
            "link_type": "co-reference",
            "shared_entities": shared[:5],  # æœ€å¤šåˆ— 5 ä¸ª
            "strength": round(strength, 2)
        })
    
    # æŒ‰å¼ºåº¦é™åºæ’åˆ—
    links.sort(key=lambda x: x["strength"], reverse=True)
    
    return links


def save_links(project_id: str, links: List[Dict]):
    path = PROJECTS_DIR / project_id / "snippets" / "links.json"
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(links, f, ensure_ascii=False, indent=2)


def load_links(project_id: str) -> List[Dict]:
    path = PROJECTS_DIR / project_id / "snippets" / "links.json"
    if not path.exists():
        return []
    with open(path, 'r', encoding='utf-8') as f:
        return json.load(f)
```

#### Relationship Analyzer å®ä½“ç±»å‹æ‰©å±•

ç°æœ‰ç±»å‹ï¼š`person | company | position`ï¼ˆé¢å‘ L-1ï¼‰

æ‰©å±•ä¸ºï¼ˆé¢å‘ EB-1Aï¼‰ï¼š

```python
# relationship_analyzer.py prompt ä¸­çš„ entity type åˆ—è¡¨
"type": "person | organization | publication | award | grant | metric | event | position"
```

å…¶ä¸­ `publication`ã€`award`ã€`grant` å¯¹ EB-1A æœ€å…³é”®â€”â€”å¾‹å¸ˆç»å¸¸éœ€è¦æŠŠè®ºæ–‡çš„å‘è¡¨è®°å½•ã€å¼•ç”¨æ•°æ®ã€å’Œæ¨èä¿¡ä¸­å¯¹è¯¥è®ºæ–‡çš„è¯„ä»·æ”¾åœ¨ä¸€èµ·ã€‚

#### è§¦å‘æ—¶æœº

åœ¨ relationship analysis å®Œæˆåè‡ªåŠ¨è°ƒç”¨ï¼š

```python
# pipeline.py ä¸­ relationship analysis å®Œæˆçš„å›è°ƒæœ«å°¾
from app.services.snippet_linker import build_snippet_links, save_links
from app.services.snippet_registry import load_registry

snippet_registry = load_registry(project_id)
graph_data = storage.load_relationship_result(project_id)
links = build_snippet_links(graph_data, snippet_registry)
save_links(project_id, links)
```

---

### Step 3ï¼šConstrained Petition Generation â€” ä¸¤æ­¥æ‹†åˆ†

**æ ¸å¿ƒæ”¹åŠ¨ï¼š** ä¸€æ¬¡ LLM è°ƒç”¨ â†’ ä¸¤æ¬¡ï¼ˆå†™ä½œ + æ ‡æ³¨åˆ†ç¦»ï¼‰

#### 3aï¼šè‡ªç”±å†™ä½œï¼ˆClaude Sonnetï¼‰

```python
async def generate_petition_prose(
    project_id: str,
    section: str,  # e.g. "scholarly_articles"
    snippet_registry: List[Dict],
    snippet_links: List[Dict]
) -> str:
    """
    Step 3a: è‡ªç”±å†™ä½œï¼Œä¸è¦æ±‚ JSONï¼Œåªè¦æ±‚å†™å¥½
    Model: Claude Sonnet
    """
    # åªä¼ å…¥å¾‹å¸ˆå·²æ˜ å°„åˆ°è¯¥ standard çš„ snippets
    relevant = [s for s in snippet_registry if s["standard_key"] == section]
    
    # æ„å»º contextï¼šæŒ‰ bundle/å…³è”åˆ†ç»„å‘ˆç°
    context = _build_structured_context(relevant, snippet_links)
    
    prompt = f"""You are a Senior Immigration Attorney writing an EB-1A petition.

Write a persuasive, well-structured paragraph (200-400 words) for the "{section}" criterion.

Use ONLY the following evidence. Do not invent any facts.

{context}

Requirements:
- Open with a legal conclusion statement
- Present primary evidence with specific facts, dates, and figures
- Include supporting context and quantitative data
- Close with a reinforcing statement
- Professional legal tone throughout
- Reference evidence naturally (e.g. "as evidenced by..." "according to...")
"""
    
    result = await call_llm_claude(prompt, model="claude-sonnet-4-20250514")
    return result  # çº¯æ–‡æœ¬æ®µè½


def _build_structured_context(
    snippets: List[Dict], 
    links: List[Dict]
) -> str:
    """
    æ„å»ºç»™å†™ä½œ LLM çš„ context
    åˆ©ç”¨ snippet links å°†ç›¸å…³ snippets åˆ†ç»„å‘ˆç°
    è®© LLM çŸ¥é“å“ªäº›è¯æ®åº”è¯¥æ”¾åœ¨ä¸€èµ·è®¨è®º
    """
    # å»ºç«‹ snippet_id â†’ snippet æ˜ å°„
    snippet_map = {s["snippet_id"]: s for s in snippets}
    snippet_ids = set(s["snippet_id"] for s in snippets)
    
    # ç”¨ links åšç®€å•èšç±»ï¼ˆUnion-Findï¼‰
    parent = {sid: sid for sid in snippet_ids}
    
    def find(x):
        while parent[x] != x:
            parent[x] = parent[parent[x]]
            x = parent[x]
        return x
    
    def union(a, b):
        ra, rb = find(a), find(b)
        if ra != rb:
            parent[ra] = rb
    
    for link in links:
        a, b = link["snippet_a"], link["snippet_b"]
        if a in snippet_ids and b in snippet_ids and link["strength"] >= 0.3:
            union(a, b)
    
    # æŒ‰ cluster åˆ†ç»„
    clusters = defaultdict(list)
    for sid in snippet_ids:
        clusters[find(sid)].append(sid)
    
    # æ ¼å¼åŒ–è¾“å‡º
    lines = []
    group_num = 1
    for root, members in clusters.items():
        if len(members) > 1:
            # æ‰¾å‡ºè¿™ç»„å…±äº«çš„å®ä½“
            shared = set()
            for link in links:
                if link["snippet_a"] in members and link["snippet_b"] in members:
                    shared.update(link.get("shared_entities", []))
            
            lines.append(f"## Evidence Group {group_num} "
                         f"(related through: {', '.join(list(shared)[:3])})")
            group_num += 1
        
        for sid in members:
            s = snippet_map[sid]
            lines.append(f'  [{s["snippet_id"]}] ({s["exhibit_id"]}, p.{s["page"]}):')
            lines.append(f'  "{s["text"]}"')
            lines.append("")
    
    return "\n".join(lines)
```

#### 3bï¼šå¥å­çº§æ ‡æ³¨ï¼ˆGPT-4o-mini strict schemaï¼‰

```python
async def annotate_sentences(
    paragraph_text: str,
    snippet_registry: List[Dict],
    section: str
) -> List[Dict]:
    """
    Step 3b: å°†è‡ªç”±æ®µè½æ‹†å¥å¹¶æ ‡æ³¨ snippet_ids
    Model: GPT-4o-mini with strict JSON schema
    """
    relevant = [s for s in snippet_registry if s["standard_key"] == section]
    
    # æ„å»º snippet reference list
    snippet_ref = "\n".join(
        f'[{s["snippet_id"]}]: "{s["text"][:150]}"'
        for s in relevant
    )
    
    prompt = f"""Split this paragraph into individual sentences and annotate each with the snippet IDs it draws from.

PARAGRAPH:
{paragraph_text}

AVAILABLE SNIPPETS:
{snippet_ref}

Rules:
1. Every factual claim MUST reference at least one snippet_id
2. ONLY use snippet_ids from the list above
3. Transitional/concluding sentences with no specific fact can have empty snippet_ids
4. Preserve the exact text â€” do not rewrite sentences
"""
    
    schema = {
        "type": "object",
        "properties": {
            "sentences": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "text": {"type": "string"},
                        "snippet_ids": {
                            "type": "array",
                            "items": {"type": "string"}
                        }
                    },
                    "required": ["text", "snippet_ids"]
                }
            }
        },
        "required": ["sentences"]
    }
    
    result = await call_llm_openai(
        prompt, 
        model="gpt-4o-mini",
        json_schema=schema  # strict mode, 100% åˆè§„
    )
    return result["sentences"]
```

#### ç»„åˆè°ƒç”¨

```python
@router.post("/write/v2/{project_id}/{section}")
async def write_petition_v2(project_id: str, section: str):
    """ä¸¤æ­¥ç”Ÿæˆï¼šå†™ä½œ + æ ‡æ³¨"""
    snippet_registry = load_registry(project_id)
    snippet_links = load_links(project_id)
    
    # 3a: å†™ä½œ
    paragraph = await generate_petition_prose(
        project_id, section, snippet_registry, snippet_links
    )
    
    # 3b: æ ‡æ³¨
    sentences = await annotate_sentences(
        paragraph, snippet_registry, section
    )
    
    # ä¿å­˜
    save_constrained_writing(project_id, section, sentences, paragraph)
    
    return {
        "success": True,
        "section": section,
        "paragraph_text": paragraph,
        "sentences": sentences
    }
```

---

### Step 4ï¼šHybrid Retrieval â€” provenance_engine.py

ä¸ v1 æ‰‹å†Œç›¸åŒï¼Œä¸å†é‡å¤ã€‚æ ¸å¿ƒé€»è¾‘ï¼š

- æ˜¾å¼ snippet_ids â†’ confidence 1.0
- Semantic/text fallback â†’ confidence Ã— 0.7
- å¾‹å¸ˆç¼–è¾‘å¥å­åè‡ªåŠ¨è§¦å‘ fallback
- ä¸éœ€è¦ GPUï¼šæ–‡æœ¬ç›¸ä¼¼åº¦ fallback è¶³å¤Ÿï¼Œembedding ä¸ºå¯é€‰å‡çº§

---

### Step 5ï¼šBBox Highlight

ä¸ v1 æ‰‹å†Œç›¸åŒï¼Œä¸å†é‡å¤ã€‚å‰ç«¯ä¸¤ä¸ªç«¯ç‚¹ï¼š

- `GET /provenance/{project_id}/sentence` â€” æ­£å‘æº¯æº
- `GET /provenance/{project_id}/reverse` â€” åå‘æº¯æº

---

## å››ã€å‰ç«¯æ”¹é€ 

### 4.1 Snippet å…³è”ä¿¡å·å±•ç¤º

åœ¨ Evidence Card Pool æˆ– Writing Canvas ä¸­ï¼Œæ˜¾ç¤º snippet é—´çš„å…³è”ï¼š

```tsx
// ç›¸å…³ snippets ä¹‹é—´ç”»ä¸€æ¡æ·¡è‰²è™šçº¿
// hover æŸä¸ª snippet æ—¶ï¼Œç›¸å…³ snippets è½»å¾®é«˜äº®
// tooltip æ˜¾ç¤º "Related through: Nature è®ºæ–‡, Dr. Chen"

interface SnippetLink {
  snippet_a: string;
  snippet_b: string;
  link_type: 'co-reference';
  shared_entities: string[];
  strength: number;  // 0-1
}

// åœ¨ EvidenceCardPool æˆ– WritingCanvas ä¸­
const LinkedSnippetIndicator: React.FC<{
  currentSnippetId: string;
  links: SnippetLink[];
  onHoverLink: (linkedIds: string[]) => void;
}> = ({ currentSnippetId, links, onHoverLink }) => {
  const relatedLinks = links.filter(
    l => l.snippet_a === currentSnippetId || l.snippet_b === currentSnippetId
  );
  
  if (relatedLinks.length === 0) return null;
  
  const linkedIds = relatedLinks.map(l => 
    l.snippet_a === currentSnippetId ? l.snippet_b : l.snippet_a
  );
  
  return (
    <div 
      className="text-xs text-gray-400 mt-1 cursor-pointer hover:text-blue-500"
      onMouseEnter={() => onHoverLink(linkedIds)}
      onMouseLeave={() => onHoverLink([])}
    >
      ğŸ”— {relatedLinks.length} related snippet(s)
      <span className="text-gray-300 ml-1">
        via {relatedLinks[0].shared_entities.slice(0, 2).join(', ')}
      </span>
    </div>
  );
};
```

**äº¤äº’åŸåˆ™ï¼šä¿¡å·è€Œéå†³ç­–ã€‚** ç³»ç»Ÿåªæä¾›è§†è§‰ä¿¡å·ï¼ˆ"è¿™ä¸¤ä¸ª snippet æåˆ°äº†åŒä¸€ç¯‡è®ºæ–‡"ï¼‰ï¼Œä¸æ›¿å¾‹å¸ˆåšåˆ†ç»„å†³ç­–ã€‚å¾‹å¸ˆçœ‹åˆ°ä¿¡å·åè‡ªå·±åˆ¤æ–­æ˜¯å¦æŠŠå®ƒä»¬æ”¾åœ¨åŒä¸€ä¸ª Argument ä¸‹ã€‚

### 4.2 Evidence Bundleï¼ˆå¯é€‰ï¼Œä½ä¼˜å…ˆçº§ï¼‰

å¦‚æœå¾‹å¸ˆè§‰å¾—å…³è”ä¿¡å·æœ‰ç”¨ï¼Œå¯ä»¥æ‰‹åŠ¨æ¡†é€‰ snippets å½¢æˆ bundleï¼š

- åœ¨ WritingCanvas ä¸­å¤šé€‰å‡ ä¸ª snippet èŠ‚ç‚¹ â†’ å³é”® "Group as bundle"
- è§†è§‰ä¸Šç”¨ä¸€ä¸ªæµ…è‰²èƒŒæ™¯æ¡†åŒ…è£¹
- bundle ä¼ ç»™ writing LLM æ—¶ä½œä¸ºä¸€ä¸ª evidence group

è¿™ä¸ªåŠŸèƒ½åœ¨ user study ä¸­ä½œä¸º available feature å­˜åœ¨ï¼Œåœ¨è®¿è°ˆ ablation ä¸­æ”¶é›†åé¦ˆï¼Œä¸ä½œä¸ºå®éªŒå˜é‡ã€‚

### 4.3 Sentence-level æº¯æºäº¤äº’

ä¸ v1 æ‰‹å†Œç›¸åŒï¼š
- WritingCanvas ä¸­æ®µè½æŒ‰å¥å­æ¸²æŸ“
- ç‚¹å‡»å¥å­ â†’ DocumentViewer é«˜äº® bbox
- ä¸åŒ snippet ä¸åŒé¢œè‰²
- æ— æº¯æºçš„è¿‡æ¸¡å¥ç°è‰²æ˜¾ç¤º

### 4.4 å‰åç«¯è¿æ¥

PetitionLetter2.0 å‰ç«¯ç›®å‰ç”¨ mock æ•°æ®ã€‚éœ€è¦ï¼š
1. `src/services/api.ts` â€” API clientï¼Œè¿æ¥åç«¯
2. `AppContext.tsx` â€” æ›¿æ¢ mock import ä¸º API è°ƒç”¨
3. å°† EB-1A 10 æ ‡å‡†å’Œ L-1 4 æ ‡å‡†åšæˆå¯é…ç½®

---

---

## äº”ã€è®ºæ–‡å¯¹é½æ£€æŸ¥

| è®ºæ–‡å£°æ˜ | å®ç° |
|---------|------|
| Step 1: Dual Indexing | `deepseek_ocr.py` â†’ TextBlock (ä¸å˜) |
| Step 2: Snippet Extraction + bbox ç»§æ‰¿ | `l1_analyzer` + `snippet_registry.py` |
| Step 3: Structured JSON, æ¯å¥æºå¸¦ snippet_ids | 3b `annotate_sentences()` strict schema |
| åªå°†å·²æ˜ å°„ snippet æ”¾å…¥ context | `_build_structured_context()` è¿‡æ»¤ |
| Step 4: æ˜¾å¼æ ‡æ³¨ä¸» + è¯­ä¹‰ fallback | `provenance_engine.py` |
| æ˜¾å¼æƒé‡ > è¯­ä¹‰æƒé‡ | confidence 1.0 vs Ã— 0.7 |
| Step 5: BBox Highlight <200ms | å‰ç«¯ canvas overlay + å†…å­˜æŸ¥è¯¢ |
| ä¸€å¥â†’å¤š snippet | `resolve_provenance()` top-5 |
| ä¸€ snippetâ†’å¤šå¥ | `/provenance/reverse` endpoint |
| deterministic + probabilistic | explicit + semantic |
| å¤šè¯æ®èšåˆæ˜¯å¸¸è§æ¨¡å¼ (DP3) | snippet_linker å…³è”ä¿¡å· + bundle UI |
| Argument ä¸­é—´å±‚ | WritingCanvas ä¸‰å±‚èŠ‚ç‚¹ (å·²æœ‰) |

---

## å…­ã€User Study å®éªŒè®¾è®¡

### æ ¸å¿ƒå¯¹æ¯”ï¼šä¸¤ç§èŒƒå¼

Condition B ä¸æ˜¯ç³»ç»Ÿçš„é˜‰å‰²ç‰ˆï¼Œè€Œæ˜¯æ¨¡æ‹Ÿå¾‹å¸ˆç°åœ¨çœŸå®çš„ AI è¾…åŠ©å·¥ä½œæµï¼ˆgenerate-then-verifyï¼‰ã€‚

```
â”Œâ”€â”€â”€ Condition Aï¼ˆExtract-then-Assembleï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                           â”‚
â”‚  [æºæ–‡æ¡£ PDF]  [Snippet å¡ç‰‡æ± ]  [Standards + Canvas]     â”‚
â”‚                                                           â”‚
â”‚  å¾‹å¸ˆå…ˆæ‹–æ‹½ snippets åˆ° standards                          â”‚
â”‚  â†’ ç³»ç»Ÿæ ¹æ®æ˜ å°„ç”Ÿæˆ petition                               â”‚
â”‚  â†’ å¾‹å¸ˆåœ¨è¿‡ç¨‹ä¸­å·²ç»ä¸»åŠ¨æ¥è§¦æ¯ä¸€æ¡è¯æ®                        â”‚
â”‚  â†’ ä»»åŠ¡ï¼šæ£€æŸ¥ petition ä¸­çš„é”™è¯¯                             â”‚
â”‚                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€ Condition Bï¼ˆGenerate-then-Verifyï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                           â”‚
â”‚  [æºæ–‡æ¡£ PDF]              [AI ç”Ÿæˆçš„ petition æ–‡æœ¬]       â”‚
â”‚                                                           â”‚
â”‚  å¾‹å¸ˆç›´æ¥æ‹¿åˆ°æˆå“ petition                                 â”‚
â”‚  â†’ æ–‡ä¸­æœ‰ [Exhibit A-1, p.3] å¯ç‚¹å‡»è·³è½¬åˆ°æºæ–‡æ¡£            â”‚
â”‚  â†’ å¾‹å¸ˆé€å¥é˜…è¯»ï¼Œé€å¥æ ¸å®                                  â”‚
â”‚  â†’ ä»»åŠ¡ï¼šæ£€æŸ¥ petition ä¸­çš„é”™è¯¯                             â”‚
â”‚                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ä¿¡æ¯é‡ç­‰ä»·

| ç»´åº¦ | Condition A | Condition B |
|------|------------|------------|
| æºæ–‡æ¡£ | âœ… ç›¸åŒ exhibits | âœ… ç›¸åŒ exhibits |
| Petition æ–‡æœ¬ | âœ… åŒä¸€ä»½ AI ç”Ÿæˆæ–‡æœ¬ | âœ… åŒä¸€ä»½ AI ç”Ÿæˆæ–‡æœ¬ |
| é¢„åŸ‹é”™è¯¯ | âœ… ç›¸åŒ 5 ä¸ªé”™è¯¯ | âœ… ç›¸åŒ 5 ä¸ªé”™è¯¯ |
| Citation å›æº¯ | å¥å­çº§ â†’ snippet â†’ bbox | inline [Exhibit A-1, p.3] â†’ PDF è·³è½¬ |
| Snippet æ±  | âœ… | âŒ |
| æ‹–æ‹½æ˜ å°„ | âœ… | âŒ |
| Argument å±‚ | âœ… | âŒ |

**å”¯ä¸€å·®å¼‚ï¼šCondition A å¾‹å¸ˆåœ¨çœ‹åˆ° petition ä¹‹å‰ï¼Œç»å†äº† assembly è¿‡ç¨‹ã€‚**

### ICAP ç†è®ºé¢„æµ‹

- Condition B = Active engagementï¼ˆæµè§ˆã€ç‚¹å‡»ã€é˜…è¯»ï¼‰
- Condition A = Constructive engagementï¼ˆä¸»åŠ¨å»ºç«‹æ˜ å°„å…³ç³»ã€ç”Ÿæˆè®ºç‚¹ç»“æ„ï¼‰
- ICAP é¢„æµ‹ï¼šConstructive > Active â†’ Condition A åº”æ£€å‡ºæ›´å¤šé”™è¯¯

### Petition æ–‡æœ¬æ¥æº

ç”¨ Condition A çš„ç³»ç»Ÿç”Ÿæˆï¼ˆä¸¤æ­¥å†™ä½œ 3a+3bï¼‰ï¼Œä¸¤ç»„å¾‹å¸ˆçœ‹åˆ°ä¸€æ¨¡ä¸€æ ·çš„æ–‡æœ¬ã€‚
Condition A å¾‹å¸ˆç»å† assembly åçœ‹åˆ°å®ƒï¼›Condition B å¾‹å¸ˆç›´æ¥çœ‹åˆ°å®ƒã€‚

### Condition B å‰ç«¯å®ç°

å·¥ç¨‹é‡å°ã€‚ä¸¤ä¸ªé¢æ¿ï¼š

```
å·¦æ ï¼šDocumentViewerï¼ˆå¤ç”¨ç°æœ‰ç»„ä»¶ï¼‰
å³æ ï¼šReadOnlyPetitionPanelï¼ˆæ–°å»ºï¼Œç®€å•ç»„ä»¶ï¼‰
  - æ¸²æŸ“ petition æ–‡æœ¬
  - inline citation [Exhibit A-1, p.3] å¯ç‚¹å‡»
  - ç‚¹å‡» â†’ å·¦æ  PDF è·³è½¬åˆ°å¯¹åº”é¡µ
  - å¾‹å¸ˆå¯ä»¥åœ¨æ–‡æœ¬ä¸­æ ‡è®°é”™è¯¯ï¼ˆé«˜äº® + æ ‡æ³¨ï¼‰
```

```tsx
const ReadOnlyPetitionPanel: React.FC<{
  sections: Array<{title: string, text: string, citations: Citation[]}>;
  onCitationClick: (exhibitId: string, page: number) => void;
  onMarkError: (sectionIndex: number, selection: string, errorType: string) => void;
}> = ({ sections, onCitationClick, onMarkError }) => {
  // æ¸²æŸ“ petition æ–‡æœ¬
  // citation ç”¨è“è‰²é“¾æ¥æ ·å¼ï¼Œç‚¹å‡»è§¦å‘ PDF è·³è½¬
  // å³é”®æˆ–å·¥å…·æ æŒ‰é’®æ ‡è®°é”™è¯¯
};
```

é¢„ä¼°å·¥æ—¶ï¼š4hï¼ˆå¤§éƒ¨åˆ†æ˜¯ citation ç‚¹å‡»è·³è½¬é€»è¾‘ï¼‰

### å®éªŒä»»åŠ¡

ä¸¤ç»„ç›¸åŒä»»åŠ¡ï¼š

> "ä»¥ä¸‹æ˜¯ AI ç”Ÿæˆçš„ EB-1A petition æ®µè½å’Œå¯¹åº”çš„æºæ–‡æ¡£ææ–™ã€‚è¯·å®¡é˜… petitionï¼Œæ‰¾å‡ºå…¶ä¸­çš„é”™è¯¯ã€‚é”™è¯¯å¯èƒ½åŒ…æ‹¬ï¼š
> - äº‹å®æ€§é”™è¯¯ï¼ˆæ•°å­—ã€æ—¥æœŸã€åç§°ä¸ä¸€è‡´ï¼‰
> - è¯æ®å¼•ç”¨é”™è¯¯ï¼ˆå¼•ç”¨äº†é”™è¯¯çš„ exhibitï¼‰
> - é—æ¼å…³é”®è¯æ®ï¼ˆæœ‰è¯æ®æœªè¢«å¼•ç”¨ï¼‰
> - é€»è¾‘é—®é¢˜ï¼ˆè¯æ®ä¸æ”¯æŒè®ºç‚¹ï¼‰
> 
> è¯·æ ‡è®°æ‰€æœ‰ä½ å‘ç°çš„é”™è¯¯ã€‚"

### é‡åŒ–æŒ‡æ ‡

| æŒ‡æ ‡ | å«ä¹‰ |
|------|------|
| Error Detection Rate | æ£€å‡ºçš„é¢„åŸ‹é”™è¯¯æ•° / 5 |
| Precision | æ­£ç¡®æ ‡è®° / æ€»æ ‡è®°æ•°ï¼ˆå«è¯¯æŠ¥ï¼‰ |
| Time to First Error | å‘ç°ç¬¬ä¸€ä¸ªé”™è¯¯çš„æ—¶é—´ |
| Total Task Time | å®Œæˆå®¡é˜…çš„æ€»æ—¶é—´ |
| False Positive Rate | é”™è¯¯æ ‡è®°çš„éé”™è¯¯æ•° |

### å®šæ€§æ•°æ®

- NASA-TLX å·¥ä½œè´Ÿè·é‡è¡¨
- åŠç»“æ„åŒ–è®¿è°ˆï¼ˆ15-20 minï¼‰
- è®¿è°ˆ ablation å—ï¼š
  > "ç³»ç»Ÿæ˜¾ç¤ºäº†ä¸€äº›è¯æ®ä¹‹é—´çš„å…³è”æç¤ºã€‚è¿™äº›æç¤ºå¯¹ä½ ç»„ç»‡è®ºè¯æœ‰å¸®åŠ©å—ï¼Ÿ"
  > "ä½ æœ‰æ²¡æœ‰æŠŠå‡ ä¸ªè¯æ®æ‰‹åŠ¨ç»„åˆåœ¨ä¸€èµ·ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ"
  > "åœ¨å®¡é˜…è¿‡ç¨‹ä¸­ï¼Œä½ æ˜¯æ€ä¹ˆå†³å®šå»æ ¸å®æŸå¥è¯çš„ï¼Ÿ"

### è¢«è¯•

- N = 6-8 åç§»æ°‘å¾‹å¸ˆï¼ˆwithin-subjects, counterbalancedï¼‰
- æ¯äººåšä¸¤ä¸ª caseï¼ˆä¸€ä¸ª Condition Aï¼Œä¸€ä¸ª Condition Bï¼‰
- Case å’Œ Condition çš„ç»„åˆ counterbalance

---

## ä¸ƒã€Technical Evaluationï¼ˆç‹¬ç«‹å®éªŒï¼ŒUser Study å‰æ‰§è¡Œï¼‰

### å®šä½

TE æœ‰ä¸¤ä¸ªä½œç”¨ï¼š
1. **Pilot test** â€” åœ¨å¾‹å¸ˆå‚ä¸å‰è·‘ä¸€éå®Œæ•´ pipelineï¼Œå‘ç° bugã€éªŒè¯å¯é æ€§
2. **é˜²å¾¡æ€§æ•°æ®** â€” è®ºæ–‡ä¸­ç”¨ 3-5 å¥è¯ + ä¸€ä¸ªå°è¡¨æ ¼æŠ¥å‘Šï¼Œé˜² reviewer è´¨ç–‘

**ä¸å•ç‹¬æˆ sectionã€‚** æ”¾åœ¨ System Design æœ«å°¾æˆ– User Study å¼€å¤´ï¼Œå çº¦å››åˆ†ä¹‹ä¸€é¡µï¼š

> *Before the user study, we validated the pipeline's technical reliability on two EB-1A cases. Snippet extraction achieved X% recall and Y% precision against expert annotations. Sentence-level provenance annotation achieved Precision@3 of Z. BBox matching yielded a mean IoU of W. These results confirmed the system was sufficiently reliable for the user study.*

### è¯„ä¼°å“ªäº›ç»„ä»¶

åªè¯„ä¼°å‡ºé”™ä¼šç ´åç”¨æˆ·ä½“éªŒçš„ç»„ä»¶ï¼š

| ç»„ä»¶ | å‡ºé”™åæœ | éœ€è¦è¯„ä¼°ï¼Ÿ |
|------|---------|-----------|
| Snippet Extraction | å¾‹å¸ˆåœ¨ snippet æ± é‡Œæ‰¾ä¸åˆ°å…³é”®è¯æ® | âœ… |
| Sentence Annotation (3b) | ç‚¹å‡»å¥å­çœ‹åˆ°é”™è¯¯çš„æºæ–‡æ¡£ä½ç½® | âœ… |
| BBox Matching | é«˜äº®æ¡†ä½ç½®åç§» | âœ… |
| Writing (3a) | petition æ–‡æœ¬è´¨é‡ | âŒ ä¸æ˜¯è´¡çŒ®ç‚¹ |
| Snippet Linking | å…³è”æç¤ºä¸å‡† | âŒ åªæ˜¯è¾…åŠ©ä¿¡å· |

### æŒ‡æ ‡

| ç»„ä»¶ | æŒ‡æ ‡ | å®šä¹‰ |
|------|------|------|
| Snippet Extraction | Recall | æ ‡æ³¨çš„æœ‰æ•ˆè¯æ®ä¸­ï¼ŒAI æå–äº†å¤šå°‘ |
| Snippet Extraction | Precision | AI æå–çš„ snippets ä¸­ï¼Œå¤šå°‘æ˜¯æœ‰æ•ˆè¯æ® |
| Sentence Annotation | Precision@3 | å‰ 3 ä¸ªæ ‡æ³¨ snippet ä¸­æœ‰å‡ ä¸ªæ­£ç¡® |
| Sentence Annotation | MRR | ç¬¬ä¸€ä¸ªæ­£ç¡® snippet æ’åœ¨ç¬¬å‡ ä½ |
| BBox Matching | IoU | ç³»ç»Ÿ bbox ä¸ ground truth bbox çš„äº¤å¹¶æ¯” |

### æ‰§è¡Œæ–¹å¼

ä¸éœ€è¦å¾‹å¸ˆå‚ä¸ï¼Œè‡ªå·±æ ‡æ³¨ ground truthã€‚

**ææ–™ï¼š** User study çš„ä¸¤å¥— EB-1A æ¡ˆä¾‹

| æ ‡æ³¨ä»»åŠ¡ | å†…å®¹ | å·¥æ—¶ |
|---------|------|------|
| Snippet Extraction GT | è¯»æºæ–‡æ¡£ï¼Œæ ‡æ³¨æœ‰æ•ˆè¯æ®ç‰‡æ®µ | 2-3h / case |
| Sentence Annotation GT | è¯» petition æ¯å¥è¯ï¼Œæ ‡æ³¨è¯¥å¯¹åº”å“ªäº› snippets | 1-2h / case |
| BBox GT | åœ¨ PDF ä¸Šç”»å‡º snippet çš„æ­£ç¡®ä½ç½®æ¡† | 1-2h / case |

ä¸¤ä¸ª case åˆè®¡ **1-2 å¤©**ã€‚

### æ—¶é—´çº¿

```
Week 1-2: ç³»ç»Ÿæ”¹é€ ï¼ˆP0 + P1ï¼‰
Week 3:   Technical Evaluation
             â†’ è·‘å®Œæ•´ pipeline
             â†’ æ ‡æ³¨ ground truthï¼Œè®¡ç®—æŒ‡æ ‡
             â†’ å‘ç°å¹¶ä¿®å¤ bugï¼ˆå…¼åš pilot testï¼‰
Week 4:   User Studyï¼ˆç³»ç»Ÿå·²éªŒè¯ï¼Œæ›´ç¨³å®šï¼‰
```

### é£é™©æç¤º

å¦‚æœæŸé¡¹æŒ‡æ ‡å¾ˆä½ï¼ˆå¦‚ BBox IoU < 0.5ï¼‰ï¼Œå…ˆä¿® bug æ‹‰é«˜å†æŠ¥å‘Šã€‚TE çš„ç›®çš„æ˜¯é˜²å¾¡ï¼Œä¸æ˜¯è‡ªæ›å¼±ç‚¹ã€‚åªæŠ¥å‘Šå¥½çœ‹çš„æ•°æ®ï¼Œå¼±é¡¹åœ¨ Limitations é‡Œä¸€ç¬”å¸¦è¿‡ã€‚

### äº¤äº’æ—¥å¿—ï¼ˆè¡¥å……æ•°æ®æºï¼‰

é™¤ç‹¬ç«‹ TE å¤–ï¼ŒCondition A å‰ç«¯ä¹Ÿåº”è®°å½•äº¤äº’æ—¥å¿—ï¼Œç”¨äºäº‹åè¡¥å……åˆ†æï¼š

```typescript
interface InteractionLog {
  timestamp: number;
  event_type: 
    | 'snippet_drag'           // æ‹–æ‹½ snippet åˆ° standard
    | 'mapping_confirm'        // ç¡®è®¤ AI æ˜ å°„ï¼ˆdashed â†’ solidï¼‰
    | 'mapping_reject'         // æ‹’ç» AI æ˜ å°„
    | 'mapping_create'         // æ‰‹åŠ¨åˆ›å»ºæ–°æ˜ å°„
    | 'sentence_click'         // ç‚¹å‡»å¥å­æŸ¥çœ‹æº¯æº
    | 'provenance_correct'     // çº æ­£æº¯æºç»“æœ
    | 'error_mark'             // æ ‡è®°é”™è¯¯
    | 'bundle_create'          // åˆ›å»º evidence bundle
    | 'bundle_modify';         // ä¿®æ”¹ bundle
  data: {
    snippet_id?: string;
    standard_key?: string;
    sentence_index?: number;
  };
}
```

æ—¥å¿—å¯æä¾›é¢å¤–çš„ TE æ•°æ®ï¼šå¾‹å¸ˆæœ€ç»ˆæ˜ å°„ vs AI åˆå§‹æ˜ å°„çš„å·®å¼‚ï¼Œä½œä¸º Snippet Extraction å’Œ Annotation æŒ‡æ ‡çš„ç¬¬äºŒæ•°æ®æºã€‚

---

## å…«ã€æ”¹é€ ä¼˜å…ˆçº§ï¼ˆæ›´æ–°ç‰ˆï¼‰

| ä¼˜å…ˆçº§ | ä»»åŠ¡ | ä¾èµ– | å·¥æ—¶ |
|--------|------|------|------|
| **P0** | snippet_registry.py (Step 0) | æ—  | 2h |
| **P0** | ä¸¤æ­¥å†™ä½œ 3a+3b (Step 3) | Step 0 | 5h |
| **P0** | call_llm_claude + call_llm_openai æ¥å£ | æ—  | 1h |
| **P1** | provenance_engine.py (Step 4) | Step 3 | 3h |
| **P1** | å‰ç«¯ sentence-level æ¸²æŸ“ + bbox è”åŠ¨ (Step 5) | Step 4 | 6h |
| **P1** | Condition B å‰ç«¯ ReadOnlyPetitionPanel | æ—  | 4h |
| **P1** | é¢„åŸ‹é”™è¯¯çš„ petition ç”Ÿæˆ + éªŒè¯ | Step 3 | 3h |
| **P1** | relationship_analyzer å®ä½“ç±»å‹æ‰©å±• | æ—  | 2h |
| **P1** | snippet_linker.py (Step 2.5) | å®ä½“æ‰©å±• + Step 0 | 3h |
| **P1.5** | **Technical Evaluationï¼ˆç‹¬ç«‹ï¼‰** | P0 + P1 å®Œæˆ | 10-16h |
| **P2** | å‰ç«¯ snippet å…³è”ä¿¡å·å±•ç¤º | Step 2.5 | 3h |
| **P2** | åå‘æº¯æº endpoint + å‰ç«¯ | Step 4 | 2h |
| **P2** | context å‹ç¼© (compress_ocr_for_extraction) | æ—  | 2h |
| **P2** | äº¤äº’æ—¥å¿—è®°å½• | P1 å®Œæˆ | 3h |
| **P3** | å‰ç«¯ evidence bundle æ‰‹åŠ¨åˆ†ç»„ | Step 2.5 | 4h |
| **P3** | EB-1A analyzer (standards åˆ‡æ¢) | æ—  | 3h |
| **P3** | å‰åç«¯ API è”è°ƒ (æ›¿æ¢ mock) | å…¨éƒ¨ | 4h |

### å…³é”®è·¯å¾„

```
Week 1-2: P0 + P1ï¼ˆç³»ç»Ÿæ”¹é€ ï¼‰
              snippet_registry â†’ ä¸¤æ­¥å†™ä½œ â†’ provenance â†’ å‰ç«¯æº¯æº â†’ Condition A
              Condition B ReadOnlyPanel
              é¢„åŸ‹é”™è¯¯ petition
              snippet_linker

Week 3:   P1.5 Technical Evaluation
              è·‘å®Œæ•´ pipeline â†’ æ ‡æ³¨ GT â†’ è®¡ç®—æŒ‡æ ‡ â†’ ä¿® bug
              å…¼åš pilot test

Week 4:   User Study
              ç³»ç»Ÿå·²éªŒè¯ï¼Œç¨³å®šè¿è¡Œ
```

**User study readyï¼š** P0 + P1 + P1.5 â‰ˆ 4 å‘¨
